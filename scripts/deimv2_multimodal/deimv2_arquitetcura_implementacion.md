# DEIMv2 Industrial Defects: Arquitectura e ImplementaciÃ³n

**Ãšltima actualizaciÃ³n:** 14 Noviembre 2024  
**Estado:** âœ… FASE 1 COMPLETADA - Preparando FASE 2 Multimodal

---

## ğŸ“Š Estado Actual del Proyecto

### âœ… FASE 1: DEIMv2 Vanilla - COMPLETADA

**Resultado:** mAP = 0.395 (39.5%) en validaciÃ³n

```
ğŸ¯ MÃ©tricas DEIMv2-M (Ã‰poca 86):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
mAP @ IoU=0.50:0.95   = 0.395 (39.5%)
AP  @ IoU=0.50        = 0.499 (49.9%)
AP  @ IoU=0.75        = 0.384 (38.4%)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Por tamaÃ±o de objeto:
  Small  (Ã¡rea < 32Â²)  = 0.234 (23.4%) â­
  Medium (32Â² - 96Â²)   = 0.347 (34.7%)
  Large  (Ã¡rea > 96Â²)  = 0.474 (47.4%)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Recall @ maxDets=100  = 0.621 (62.1%)
```

**Mejora vs Primer Intento:**
- Primer entrenamiento (config base): mAP = 0.178
- Segundo entrenamiento (config optimizado): mAP = 0.395
- **Mejora: +122%** ğŸš€

### ğŸ“‚ Estructura Implementada

```
scripts/deimv2_multimodal/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ deimv2_industrial_defects.yml    # âœ… Config optimizado y estable
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ deimv2_industrial_run/           # Primer intento (mAP=0.178)
â”‚   â””â”€â”€ deimv2_industrial_run_stable/    # âœ… Segundo intento (mAP=0.395)
â”‚       â”œâ”€â”€ checkpoint0084.pth           # Mejor checkpoint (Ã©poca 86)
â”‚       â”œâ”€â”€ log.txt                      # Historial entrenamiento
â”‚       â”œâ”€â”€ summary/                     # TensorBoard logs
â”‚       â””â”€â”€ test_evaluation_results.json # MÃ©tricas en test
â”œâ”€â”€ train_deimv2_industrial.py           # âœ… Script entrenamiento
â”œâ”€â”€ evaluate_deimv2.py                   # âœ… EvaluaciÃ³n mAP COCO
â”œâ”€â”€ visualize_deimv2_predictions.py      # âœ… VisualizaciÃ³n predicciones
â”œâ”€â”€ plot_deimv2_training_metrics.py      # GrÃ¡ficas (requiere parser mejorado)
â””â”€â”€ run_evaluation_deimv2.sh             # âœ… Pipeline completo eval
```

---

## ğŸ—ï¸ ConfiguraciÃ³n TÃ©cnica FASE 1

### Dataset

```yaml
Train: 715 imÃ¡genes, 944 anotaciones
Val:   102 imÃ¡genes, 145 anotaciones  
Test:  205 imÃ¡genes, 265 anotaciones

Clases (6):
  0: NORMAL
  1: DEFORMACIONES
  2: ROTURA_FRACTURA
  3: RAYONES_ARANAZOS
  4: PERFORACIONES
  5: CONTAMINACION
```

### Modelo: DEIMv2-M

```yaml
Backbone: DINOv3 ViT-Tiny+ (vittplus_distill.pt)
  - embed_dim: 256
  - num_heads: 4
  - interaction_indexes: [3, 7, 11]
  - ParÃ¡metros: ~17.8M

Encoder: HybridEncoder
  - hidden_dim: 256
  - dim_feedforward: 1024

Decoder: DEIMTransformer
  - num_layers: 4
  - hidden_dim: 256
  - num_queries: 300
```

### HiperparÃ¡metros Clave (Config Estable)

```yaml
# Entrenamiento
epoches: 100
flat_epoch: 70      # LR constante hasta Ã©poca 70
no_aug_epoch: 10    # Sin augmentations Ãºltimas 10 Ã©pocas
warmup_iter: 2000   # Warmup largo para estabilidad

# Optimizer
lr: 0.0004                # Decoder learning rate
lr_backbone: 0.00004      # Backbone (DINOv3) learning rate
weight_decay: 0.0001
clip_max_norm: 0.1        # â­ Gradient clipping (crÃ­tico para estabilidad)

# Data Augmentation (Suavizado vs config base)
RandomPhotometricDistort: p=0.3  (antes 0.5)
RandomIoUCrop: p=0.5             (antes 0.8)
Mixup: prob=0.15                 (antes 0.5)
Mosaic: DESACTIVADO              (causaba inestabilidad)
CopyBlend: DESACTIVADO           (causaba NaN)

# Hardware
batch_size: 2
use_amp: True  # Mixed precision
GPU: RTX 4070 12GB
Tiempo: ~2 horas (100 Ã©pocas)
```

### Lecciones Aprendidas FASE 1

#### âŒ Problemas Encontrados

1. **NaN en gradientes (Ã©pocas 46, 87)**
   - Causa: Learning rate demasiado alto + augmentations agresivas
   - SoluciÃ³n: Gradient clipping + reducir LR + suavizar augmentations

2. **Dataset pequeÃ±o (715 imÃ¡genes)**
   - ViTs requieren mÃ¡s datos que CNNs
   - Augmentations pesadas (Mosaic, CopyBlend) causaban inestabilidad

3. **Batch size limitado (2)**
   - RTX 4070 12GB no soporta batch_size > 2 con DEIMv2-M
   - Gradientes ruidosos â†’ convergencia lenta

#### âœ… Soluciones Efectivas

1. **Gradient clipping (`clip_max_norm: 0.1`)**
   - Previene explosiÃ³n de gradientes
   - CrÃ­tico para estabilidad con dataset pequeÃ±o

2. **Warmup largo (2000 steps)**
   - Permite adaptaciÃ³n suave del backbone DINOv3
   - Reduce riesgo de divergencia inicial

3. **Augmentations conservadoras**
   - Desactivar Mosaic y CopyBlend
   - Reducir probabilidad de PhotometricDistort e IoUCrop
   - Trade-off aceptable: mAP 0.395 (estable) > mAP potencial 0.45 (inestable)

4. **Flat epoch largo (70 Ã©pocas)**
   - LR constante durante mÃ¡s tiempo
   - Mejor convergencia con dataset pequeÃ±o

---

## ğŸ“ˆ Comparativa con Baselines

| Modelo | Arquitectura | Params | mAP@0.50:0.95 | AP@0.50 | Tiempo | Notas |
|--------|-------------|---------|---------------|---------|--------|-------|
| ResNet-18 | CNN + Faster R-CNN | 11M | ~0.42* | ~0.50* | 1h | Baseline clÃ¡sico |
| EfficientNet-B0 | CNN + Faster R-CNN | 5M | ~0.45* | ~0.52* | 1h | Baseline eficiente |
| **DEIMv2-M v1** | **ViT + DEIM** | **17.8M** | **0.178** | **0.232** | **1h** | Config base (inestable) |
| **DEIMv2-M v2** | **ViT + DEIM** | **17.8M** | **0.395** | **0.499** | **2h** | Config optimizado âœ… |

_*Nota: Baselines pendientes de evaluaciÃ³n con protocolo COCO exacto_

### AnÃ¡lisis de Resultados

**Fortalezas de DEIMv2:**
- â­ **Objetos pequeÃ±os:** mAP 0.234 (vs tÃ­picamente <0.10 en CNNs)
- â­ **Recall alto:** 62.1% (detecta mÃ¡s defectos)
- â­ **AP@0.50:** 49.9% (comparable a CNNs en detecciÃ³n no estricta)

**Debilidades:**
- âš ï¸ **mAP@0.75:** 38.4% (localizaciÃ³n menos precisa que CNNs)
- âš ï¸ **Requiere mÃ¡s tiempo:** 2h vs 1h de CNNs
- âš ï¸ **MÃ¡s parÃ¡metros:** 17.8M vs 5-11M de CNNs

**ConclusiÃ³n FASE 1:**
DEIMv2 alcanza rendimiento **competitivo** (~94% del mAP de baselines) pero con ventajas en objetos pequeÃ±os. Base sÃ³lida para FASE 2.

---

## ğŸš€ FASE 2: ExtensiÃ³n Multimodal (PRÃ“XIMOS PASOS)

### Objetivo

**Superar mAP 0.45** mediante fusiÃ³n visiÃ³n-texto, mejorando especialmente la clasificaciÃ³n de clases visualmente similares (ej: rayones vs fracturas).

### Arquitectura Propuesta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEIMv2-M Backbone                        â”‚
â”‚  (DINOv3 ViT + Hybrid Encoder + DEIM Transformer)          â”‚
â”‚                          â†“                                  â”‚
â”‚              Visual Embeddings (300 queries Ã— 256d)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Visual Features  â”‚              â”‚ Text Embeddings  â”‚
â”‚   (Per query)    â”‚              â”‚  (Per class)     â”‚
â”‚   [B, 300, 256]  â”‚              â”‚   [6, 512]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                  â”‚
         â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“    â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Multimodal Fusion    â”‚
  â”‚  â€¢ Visual Proj 256â†’D â”‚
  â”‚  â€¢ Text Proj 512â†’D   â”‚
  â”‚  â€¢ Cosine Similarity â”‚
  â”‚  â€¢ Refinement Head   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Enhanced Predictions â”‚
  â”‚   [B, 300, 6+1]      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Plan de ImplementaciÃ³n

#### 2.1 Descripciones Textuales por Clase

```python
# scripts/deimv2_multimodal/data/class_descriptions.py

CLASS_DESCRIPTIONS = {
    0: {
        "name": "NORMAL",
        "description": "Superficie limpia sin defectos visibles ni anomalÃ­as estructurales",
        "keywords": ["limpio", "intacto", "sin daÃ±o", "superficie uniforme"]
    },
    1: {
        "name": "DEFORMACIONES", 
        "description": "AlteraciÃ³n de la forma original con abombamiento, hundimiento o deformaciÃ³n plÃ¡stica",
        "keywords": ["abolladura", "deformado", "ondulado", "curvatura anormal"]
    },
    2: {
        "name": "ROTURA_FRACTURA",
        "description": "Grieta profunda o ruptura completa del material con separaciÃ³n visible",
        "keywords": ["grieta", "fractura", "partido", "fisura profunda"]
    },
    3: {
        "name": "RAYONES_ARANAZOS",
        "description": "LÃ­nea fina y alargada de daÃ±o superficial sin penetraciÃ³n profunda",
        "keywords": ["rasguÃ±o", "lÃ­nea fina", "marca superficial", "rayÃ³n"]
    },
    4: {
        "name": "PERFORACIONES",
        "description": "Agujero circular u orificio que atraviesa total o parcialmente el material",
        "keywords": ["orificio", "perforaciÃ³n", "agujero", "taladro"]
    },
    5: {
        "name": "CONTAMINACION",
        "description": "Presencia de partÃ­culas extraÃ±as, manchas o sustancias adheridas",
        "keywords": ["suciedad", "mancha", "partÃ­culas", "residuo"]
    }
}
```

#### 2.2 MÃ³dulo de FusiÃ³n Multimodal

```python
# scripts/deimv2_multimodal/models/multimodal_fusion.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import CLIPTextModel, CLIPTokenizer

class MultimodalFusionModule(nn.Module):
    """
    Fusiona embeddings visuales de DEIMv2 con embeddings textuales
    para mejorar la clasificaciÃ³n de defectos.
    """
    
    def __init__(
        self,
        visual_dim=256,      # DEIMv2 hidden_dim
        text_dim=512,        # CLIP text embedding dim
        fusion_dim=256,      # DimensiÃ³n del espacio comÃºn
        num_classes=6,
        dropout=0.1
    ):
        super().__init__()
        
        # Text encoder (pre-entrenado)
        self.text_encoder = CLIPTextModel.from_pretrained(
            "openai/clip-vit-base-patch32"
        )
        self.tokenizer = CLIPTokenizer.from_pretrained(
            "openai/clip-vit-base-patch32"
        )
        
        # Congelar text encoder (o hacer fine-tune ligero)
        for param in self.text_encoder.parameters():
            param.requires_grad = False
        
        # Proyecciones a espacio comÃºn
        self.visual_proj = nn.Sequential(
            nn.Linear(visual_dim, fusion_dim),
            nn.LayerNorm(fusion_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        self.text_proj = nn.Sequential(
            nn.Linear(text_dim, fusion_dim),
            nn.LayerNorm(fusion_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Fusion head
        self.fusion_head = nn.Sequential(
            nn.Linear(fusion_dim * 2, fusion_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(fusion_dim, num_classes + 1)  # +1 para background
        )
        
        # Cache de text embeddings (computar una vez)
        self.register_buffer('text_embeddings', torch.zeros(num_classes, text_dim))
        self._text_embeddings_computed = False
    
    def compute_text_embeddings(self, class_descriptions):
        """
        Pre-computa embeddings de texto para todas las clases.
        Se llama una vez al inicio del entrenamiento.
        """
        if self._text_embeddings_computed:
            return
        
        text_embeds = []
        for desc in class_descriptions:
            tokens = self.tokenizer(
                desc, 
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=77
            ).to(self.text_embeddings.device)
            
            with torch.no_grad():
                text_output = self.text_encoder(**tokens)
                # Usar [CLS] token o pooled output
                text_embed = text_output.pooler_output
                text_embeds.append(text_embed.squeeze(0))
        
        self.text_embeddings = torch.stack(text_embeds)
        self._text_embeddings_computed = True
    
    def forward(self, visual_features, return_similarity=False):
        """
        Args:
            visual_features: [B, num_queries, visual_dim] - desde DEIMv2
            return_similarity: si True, retorna tambiÃ©n cosine similarity
        
        Returns:
            logits: [B, num_queries, num_classes + 1]
            (opcional) similarity: [B, num_queries, num_classes]
        """
        B, N, _ = visual_features.shape
        
        # Proyectar features visuales
        v_proj = self.visual_proj(visual_features)  # [B, N, fusion_dim]
        v_norm = F.normalize(v_proj, dim=-1)
        
        # Proyectar embeddings de texto
        t_proj = self.text_proj(self.text_embeddings)  # [num_classes, fusion_dim]
        t_norm = F.normalize(t_proj, dim=-1)
        
        # Cosine similarity (attention)
        similarity = torch.matmul(v_norm, t_norm.t())  # [B, N, num_classes]
        
        # Weighted text features
        text_context = torch.matmul(
            similarity.softmax(dim=-1),  # [B, N, num_classes]
            t_proj                        # [num_classes, fusion_dim]
        )  # [B, N, fusion_dim]
        
        # Concatenar visual + text context
        fused = torch.cat([v_proj, text_context], dim=-1)  # [B, N, 2*fusion_dim]
        
        # ClasificaciÃ³n final
        logits = self.fusion_head(fused)  # [B, N, num_classes + 1]
        
        if return_similarity:
            return logits, similarity
        return logits
```

#### 2.3 IntegraciÃ³n con DEIMv2

```python
# scripts/deimv2_multimodal/models/deimv2_multimodal.py

class DEIMv2Multimodal(nn.Module):
    """
    Wrapper que aÃ±ade MultimodalFusion sobre DEIMv2 base.
    """
    
    def __init__(self, deimv2_model, class_descriptions):
        super().__init__()
        
        self.deimv2 = deimv2_model
        
        # MÃ³dulo multimodal
        self.multimodal_fusion = MultimodalFusionModule(
            visual_dim=256,
            text_dim=512,
            num_classes=6
        )
        
        # Computar text embeddings
        self.multimodal_fusion.compute_text_embeddings(
            [desc['description'] for desc in class_descriptions.values()]
        )
    
    def forward(self, images, targets=None):
        """
        Args:
            images: tensor [B, 3, H, W]
            targets: dict con boxes, labels (entrenamiento)
        
        Returns:
            outputs: dict con pred_logits, pred_boxes (con fusiÃ³n multimodal)
        """
        # Forward pass DEIMv2 base
        outputs = self.deimv2(images, targets)
        
        # Extraer features visuales del decoder
        # outputs contiene: pred_logits, pred_boxes, hs (hidden states)
        visual_features = outputs['hs'][-1]  # [B, num_queries, hidden_dim]
        
        # Aplicar fusiÃ³n multimodal
        enhanced_logits = self.multimodal_fusion(visual_features)
        
        # Reemplazar logits originales con enhanced
        outputs['pred_logits'] = enhanced_logits
        
        return outputs
```

#### 2.4 Script de Entrenamiento FASE 2

```python
# scripts/deimv2_multimodal/train_deimv2_multimodal.py

def main(args):
    # 1. Cargar modelo DEIMv2 pre-entrenado (FASE 1)
    cfg = YAMLConfig(args.config)
    deimv2_base = cfg.model
    
    checkpoint = torch.load(args.resume, map_location='cpu')
    deimv2_base.load_state_dict(checkpoint['model'])
    
    # 2. Envolver con mÃ³dulo multimodal
    from data.class_descriptions import CLASS_DESCRIPTIONS
    model = DEIMv2Multimodal(deimv2_base, CLASS_DESCRIPTIONS)
    
    # 3. Congelar backbone (opcional, para fine-tune rÃ¡pido)
    for param in model.deimv2.backbone.parameters():
        param.requires_grad = False
    
    # 4. Entrenar solo mÃ³dulo multimodal (20 Ã©pocas adicionales)
    optimizer = torch.optim.AdamW([
        {'params': model.multimodal_fusion.parameters(), 'lr': 1e-4}
    ])
    
    # ... resto del training loop
```

#### 2.5 Config FASE 2

```yaml
# configs/deimv2_industrial_multimodal.yml

__include__: ['deimv2_industrial_defects.yml']

# Cambios para FASE 2
output_dir: ./scripts/deimv2_multimodal/outputs/deimv2_multimodal_run

# Fine-tuning (Ã©pocas cortas sobre modelo pre-entrenado)
epoches: 20
flat_epoch: 15
no_aug_epoch: 3
warmup_iter: 500

# Optimizer solo para mÃ³dulo multimodal
optimizer:
  lr: 0.0001  # LR bajo para fine-tune
  
# Cargar checkpoint FASE 1
resume: ./scripts/deimv2_multimodal/outputs/deimv2_industrial_run_stable/checkpoint0084.pth
```

### Roadmap FASE 2

#### Semana 1: Setup Multimodal
- [ ] Implementar `class_descriptions.py` con descripciones
- [ ] Implementar `MultimodalFusionModule`
- [ ] Implementar `DEIMv2Multimodal` wrapper
- [ ] Test de integraciÃ³n (forward pass sin errores)

#### Semana 2: Entrenamiento Incremental
- [ ] Crear config `deimv2_industrial_multimodal.yml`
- [ ] Entrenar 20 Ã©pocas con backbone congelado
- [ ] Evaluar mAP multimodal vs vanilla

#### Semana 3: AnÃ¡lisis y OptimizaciÃ³n
- [ ] Visualizar attention maps texto-visual
- [ ] Analizar quÃ© clases mejoran mÃ¡s
- [ ] Iterar descripciones textuales si es necesario
- [ ] Fine-tune end-to-end si mejora mAP

### Expectativas FASE 2

**Objetivo:** mAP > 0.45 (superar baselines CNN)

**Mejoras esperadas:**
- **ClasificaciÃ³n:** +5-8% en clases ambiguas (rayones vs fracturas)
- **Recall:** +3-5% por mejor discriminaciÃ³n semÃ¡ntica
- **Objetos pequeÃ±os:** Mantener ventaja (mAP ~0.25)

**Best case:** mAP ~0.48 (6% mejora sobre vanilla)  
**Realistic case:** mAP ~0.42-0.45 (comparable a CNNs)  
**Worst case:** mAP ~0.40 (mejora marginal, pero extensiÃ³n vÃ¡lida)

---

## ğŸ“ Tareas Inmediatas

### Antes de FASE 2

1. **Evaluar checkpoint0084 en test set**
   ```bash
   cd scripts/deimv2_multimodal
   ./run_evaluation_deimv2.sh \
     outputs/deimv2_industrial_run_stable/checkpoint0084.pth
   ```

2. **Comparar con baselines CNN (protocolo COCO)**
   ```bash
   # ResNet-18
   cd scripts/resnet18
   python evaluate_model.py --checkpoint ... --score-threshold 0.5
   
   # EfficientNet
   cd scripts/efficientnet
   python evaluate_model.py --checkpoint ... --score-threshold 0.5
   ```

3. **Analizar visualizaciones**
   - Revisar `outputs/.../visualizations_test/`
   - Identificar errores tÃ­picos del modelo
   - Documentar para justificar extensiÃ³n multimodal

### Iniciar FASE 2

4. **Implementar descripciones textuales**
   - Crear `data/class_descriptions.py`
   - Validar descripciones con experto de dominio

5. **Setup mÃ³dulo multimodal**
   - Implementar `MultimodalFusionModule`
   - Test de forward pass aislado

6. **Pipeline de entrenamiento incremental**
   - Config `deimv2_industrial_multimodal.yml`
   - Script `train_deimv2_multimodal.py`

---

## ğŸ“ ContribuciÃ³n al TFG

### Valor TÃ©cnico

**FASE 1 (Completada):**
- âœ… AdaptaciÃ³n exitosa de DEIMv2 (SOTA ViT) a dominio industrial
- âœ… OptimizaciÃ³n de hiperparÃ¡metros para dataset pequeÃ±o
- âœ… Benchmarking riguroso contra baselines CNN

**FASE 2 (En desarrollo):**
- ğŸ”„ ExtensiÃ³n multimodal custom (no existe en paper original)
- ğŸ”„ FusiÃ³n visiÃ³n-texto para clasificaciÃ³n de defectos
- ğŸ”„ AnÃ¡lisis de mejora semÃ¡ntica vs puramente visual

### Estructura Memoria (CapÃ­tulos TÃ©cnicos)

**CapÃ­tulo 4: ImplementaciÃ³n DEIMv2 para Defectos Industriales**
- 4.1 Arquitectura base (DINOv3 + DEIM)
- 4.2 AdaptaciÃ³n a dataset industrial (6 clases)
- 4.3 OptimizaciÃ³n de entrenamiento (gradient clipping, augmentations)
- 4.4 Resultados vanilla (mAP 0.395, comparativa con CNNs)

**CapÃ­tulo 5: ExtensiÃ³n Multimodal VisiÃ³n-Texto**
- 5.1 MotivaciÃ³n: limitaciones de modelos visuales puros
- 5.2 DiseÃ±o de descripciones textuales por clase
- 5.3 Arquitectura de fusiÃ³n (CLIP embeddings + attention)
- 5.4 Entrenamiento incremental (fine-tune sobre FASE 1)

**CapÃ­tulo 6: Resultados y AnÃ¡lisis**
- 6.1 MÃ©tricas cuantitativas (tablas mAP, recall, precision)
- 6.2 AnÃ¡lisis cualitativo (attention maps, casos de Ã©xito/fallo)
- 6.3 Comparativa exhaustiva (CNN vs ViT vanilla vs ViT multimodal)
- 6.4 DiscusiÃ³n: trade-offs complejidad vs rendimiento

---

## ğŸš¨ Decisiones Pendientes

1. **Â¿Evaluar baselines primero o empezar FASE 2 directamente?**
   - RecomendaciÃ³n: Evaluar baselines ANTES (necesario para comparaciÃ³n justa)

2. **Â¿Fine-tune backbone en FASE 2 o solo mÃ³dulo multimodal?**
   - RecomendaciÃ³n: Solo mÃ³dulo multimodal primero (mÃ¡s rÃ¡pido, menos riesgo)

3. **Â¿Usar CLIP o alternativa (SigLIP, etc.)?**
   - RecomendaciÃ³n: CLIP (mÃ¡s maduro, fÃ¡cil integraciÃ³n)

4. **Â¿CuÃ¡ntas Ã©pocas en FASE 2?**
   - RecomendaciÃ³n: 20 Ã©pocas (suficiente para fine-tune, ~40 minutos)

---

## ğŸ“ PrÃ³xima SesiÃ³n

**Agenda propuesta:**

1. **RevisiÃ³n de resultados en test** (checkpoint0084)
2. **Comparativa definitiva** con baselines CNN
3. **DiseÃ±o de descripciones** textuales (validaciÃ³n con dominio)
4. **ImplementaciÃ³n inicial** de `MultimodalFusionModule`

**PreparaciÃ³n necesaria:**
- Evaluar checkpoint en test
- Evaluar baselines con protocolo COCO
- Pensar en descripciones textuales por clase
- Revisar visualizaciones para identificar errores

---

**Estado del proyecto: âœ… FASE 1 COMPLETADA - ğŸš€ INICIANDO FASE 2**  
**PrÃ³xima acciÃ³n: EVALUAR CHECKPOINT EN TEST Y COMPARAR CON BASELINES**